{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This Notebook scrapes company data from the Bundesanzeiger / Federal Gazette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marco/opt/anaconda3/envs/env_3_9/lib/python3.9/site-packages/requests/__init__.py:109: RequestsDependencyWarning: urllib3 (1.26.15) or chardet (None)/charset_normalizer (3.1.0) doesn't match a supported version!\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#only for jupyter\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "#AWS native\n",
    "import asyncio\n",
    "import csv\n",
    "import datetime\n",
    "import json\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import time\n",
    "from io import BytesIO\n",
    "import dateparser\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import hashlib\n",
    "import json\n",
    "import deutschland.bundesanzeiger.model\n",
    "from deutschland.config import Config, module_config\n",
    "from validateND import validate_north_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *****************************************************************************/\n",
    "# *    The code is a modification of existing code of the bundesanzeiger function from the deutschland package\n",
    "# *    Authors: Nico Duldhardt and Friedrich SchÃ¶ne\n",
    "# *    Date: 2020\n",
    "# *    Availability: https://github.com/bundesAPI/deutschland / https://av.tib.eu/media/52366\n",
    "# ****************************************************************************/\n",
    "\n",
    "class Report:\n",
    "    __slots__ = [\"date\", \"name\", \"content_url\", \"company\", \"report\"]\n",
    "\n",
    "    def __init__(self, date, name, content_url, company, report=None):\n",
    "        self.date = date\n",
    "        self.name = name\n",
    "        self.content_url = content_url\n",
    "        self.company = company\n",
    "        self.report = report\n",
    "\n",
    "    def to_dict(self):\n",
    "        return {\n",
    "            \"date\": self.date,\n",
    "            \"name\": self.name,\n",
    "            \"company\": self.company,\n",
    "            \"report\": self.report,\n",
    "        }\n",
    "\n",
    "    def to_hash(self):\n",
    "            \"\"\"MD5 hash of a the report.\"\"\"\n",
    "\n",
    "            dhash = hashlib.md5()\n",
    "\n",
    "            entry = {\n",
    "                \"date\": self.date.isoformat(),\n",
    "                \"name\": self.name,\n",
    "                \"company\": self.company,\n",
    "                \"report\": self.report,\n",
    "            }\n",
    "\n",
    "            encoded = json.dumps(entry, sort_keys=True).encode('utf-8')\n",
    "            dhash.update(encoded)\n",
    "\n",
    "            return dhash.hexdigest()\n",
    "\n",
    "\n",
    "class Bundesanzeiger:\n",
    "    __slots__ = [\"session\", \"model\", \"captcha_callback\", \"_config\"]\n",
    "\n",
    "    def __init__(self, on_captach_callback=None, config: Config = None):\n",
    "        if config is None:\n",
    "            self._config = module_config\n",
    "        else:\n",
    "            self._config = config\n",
    "\n",
    "        self.session = requests.Session()\n",
    "        if self._config.proxy_config is not None:\n",
    "            self.session.proxies.update(self._config.proxy_config)\n",
    "        if on_captach_callback:\n",
    "            self.callback = on_captach_callback\n",
    "        else:\n",
    "            import deutschland.bundesanzeiger.model\n",
    "\n",
    "            self.model = deutschland.bundesanzeiger.model.load_model()\n",
    "            self.captcha_callback = self.__solve_captcha\n",
    "\n",
    "    def __solve_captcha(self, image_data: bytes):\n",
    "\n",
    "        image = BytesIO(image_data)\n",
    "        image_arr = deutschland.bundesanzeiger.model.load_image_arr(image)\n",
    "        image_arr = image_arr.reshape((1, 50, 250, 1)).astype(np.float32)\n",
    "\n",
    "        prediction = self.model.run(None, {\"captcha\": image_arr})[0][0]\n",
    "        prediction_str = deutschland.bundesanzeiger.model.prediction_to_str(prediction)\n",
    "\n",
    "        return prediction_str\n",
    "\n",
    "    def __is_captcha_needed(self, entry_content: str):\n",
    "        soup = BeautifulSoup(entry_content, \"html.parser\")\n",
    "        return not bool(soup.find(\"div\", {\"class\": \"publication_container\"}))\n",
    "\n",
    "    def __find_all_entries_on_page(self, page_content: str, search_name: str):\n",
    "        soup = BeautifulSoup(page_content, \"html.parser\")\n",
    "        wrapper = soup.find(\"div\", {\"class\": \"result_container\"})\n",
    "        rows = wrapper.find_all(\"div\", {\"class\": \"row\"})\n",
    "        for row in rows:\n",
    "            info_element = row.find(\"div\", {\"class\": \"info\"})\n",
    "\n",
    "            if not info_element:\n",
    "                continue\n",
    "\n",
    "            link_element = info_element.find(\"a\")\n",
    "            if not link_element:\n",
    "                continue\n",
    "\n",
    "            entry_link = link_element.get(\"href\")\n",
    "            entry_name = link_element.contents[0].strip()\n",
    "\n",
    "            date_element = row.find(\"div\", {\"class\": \"date\"})\n",
    "            if not date_element:\n",
    "                continue\n",
    "\n",
    "            date = dateparser.parse(date_element.contents[0], languages=[\"de\"])\n",
    "\n",
    "            company_name_element = row.find(\"div\", {\"class\": \"first\"})\n",
    "            if not date_element:\n",
    "                continue\n",
    "\n",
    "            company_name = company_name_element.contents[0].strip()\n",
    "\n",
    "            if not company_name.lower() == search_name.lower():  # match report name with the search name \n",
    "                continue\n",
    "\n",
    "            if not date >= datetime.datetime(2019, 1, 1, 0, 0):  # only store reports that were uploaded after 2018 \n",
    "                continue\n",
    "\n",
    "            yield Report(date, entry_name, entry_link, company_name)\n",
    "\n",
    "    def __generate_result(self, content: str, company_name: str):\n",
    "        \"\"\"iterate trough all results and try to fetch single reports\"\"\"\n",
    "        result = {}\n",
    "        for element in self.__find_all_entries_on_page(content, company_name):\n",
    "            get_element_response = self.session.get(element.content_url)\n",
    "            if self.__is_captcha_needed(get_element_response.text):\n",
    "                soup = BeautifulSoup(get_element_response.text, \"html.parser\")\n",
    "                captcha_image_src = soup.find(\"div\", {\"class\": \"captcha_wrapper\"}).find(\n",
    "                    \"img\"\n",
    "                )[\"src\"]\n",
    "                img_response = self.session.get(captcha_image_src)\n",
    "                captcha_result = self.captcha_callback(img_response.content)\n",
    "                captcha_endpoint_url = soup.find_all(\"form\")[1][\"action\"]\n",
    "                get_element_response = self.session.post(\n",
    "                    captcha_endpoint_url,\n",
    "                    data={\"solution\": captcha_result, \"confirm-button\": \"OK\"},\n",
    "                )\n",
    "\n",
    "            content_soup = BeautifulSoup(get_element_response.text, \"html.parser\")\n",
    "            content_element = content_soup.find(\n",
    "                \"div\", {\"class\": \"publication_container\"}\n",
    "            )\n",
    "\n",
    "            if not content_element:\n",
    "                continue\n",
    "\n",
    "            element.report = str(content_element)\n",
    "\n",
    "            result[element.name] = element.to_dict()\n",
    "\n",
    "\n",
    "        return result\n",
    "\n",
    "    def get_reports(self, company_name: str):\n",
    "        \"\"\"\n",
    "        fetch all reports for this company name\n",
    "        :param company_name:\n",
    "        :return\" : \"Dict of all reports\n",
    "        \"\"\"\n",
    "        self.session.cookies[\"cc\"] = \"1628606977-805e172265bfdbde-10\"\n",
    "        self.session.headers.update(\n",
    "            {\n",
    "                \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\",\n",
    "                \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "                \"Accept-Language\": \"de-DE,de;q=0.9,en-US;q=0.8,en;q=0.7,et;q=0.6,pl;q=0.5\",\n",
    "                \"Cache-Control\": \"no-cache\",\n",
    "                \"Connection\": \"keep-alive\",\n",
    "                \"DNT\": \"1\",\n",
    "                \"Host\": \"www.bundesanzeiger.de\",\n",
    "                \"Pragma\": \"no-cache\",\n",
    "                \"Referer\": \"https://www.bundesanzeiger.de/\",\n",
    "                \"sec-ch-ua-mobile\": \"?0\",\n",
    "                \"Sec-Fetch-Dest\": \"document\",\n",
    "                \"Sec-Fetch-Mode\": \"navigate\",\n",
    "                \"Sec-Fetch-Site\": \"same-origin\",\n",
    "                \"Sec-Fetch-User\": \"?1\",\n",
    "                \"Upgrade-Insecure-Requests\": \"1\",\n",
    "                \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.131 Safari/537.36\",\n",
    "                \"From\": \"217614@mds.hertie-school.org\"\n",
    "            }\n",
    "        )\n",
    "        # get the jsessionid cookie\n",
    "        response = self.session.get(\"https://www.bundesanzeiger.de\")\n",
    "        # go to the start page\n",
    "        response = self.session.get(\"https://www.bundesanzeiger.de/pub/de/start?0\")\n",
    "        # perform the search\n",
    "        response = self.session.get(\n",
    "            f\"https://www.bundesanzeiger.de/pub/de/start?0-2.-top%7Econtent%7Epanel-left%7Ecard-form=&fulltext={company_name}&area_select=22&search_button=Suchen\"\n",
    "        )\n",
    "        return self.__generate_result(response.text, company_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping time: 3340.04 seconds.\n"
     ]
    }
   ],
   "source": [
    "class DateTimeEncoder(json.JSONEncoder):\n",
    "        #Override the default method\n",
    "        def default(self, obj):\n",
    "            if isinstance(obj, (datetime.date, datetime.datetime)):\n",
    "                return obj.isoformat()\n",
    "            \n",
    "def save_data(data,item): # store reports for each company as JSON\n",
    "    with open(\"data/financial_information/\"+item+\".json\", \"w\",encoding='UTF-8') as write_file:\n",
    "        json.dump(data, write_file, indent=4, cls=DateTimeEncoder)\n",
    "\n",
    "def scrape(item):\n",
    "    if item[3]=='': # validation that item hasn't bee scraped yet\n",
    "        ba = Bundesanzeiger()\n",
    "        try:\n",
    "            data = ba.get_reports(item[1]) # call scrape function with disclosed name\n",
    "            if data:\n",
    "                save_data(data,item[0])\n",
    "                return \n",
    "\n",
    "        except Exception as e:            \n",
    "            pass  \n",
    "\n",
    "        try:\n",
    "            name = validate_north_data(item[1],item[2]) # Try to validate company name externally with register number\n",
    "            if name:\n",
    "                ba2 = Bundesanzeiger()\n",
    "                data = ba2.get_reports(str(name)) # call scrape function with validated name\n",
    "                if data:\n",
    "                    save_data(data,item[0])\n",
    "                    return\n",
    "                \n",
    "        except Exception as e:            \n",
    "            pass\n",
    "\n",
    "\n",
    "\n",
    "async def get_data_asynchronous():\n",
    "    start_time = time.time()\n",
    "    tasks = []\n",
    "    \n",
    "    with open('data/main/companies.csv') as file:    #Files contain all beneficiaries of government aid\n",
    "        csv_reader = list(csv.reader(file, delimiter='$'))[0:10] ## stopping after 10 companies (for test runs)\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=3) as executor:  ## set workers asynchronous calls\n",
    "\n",
    "            loop = asyncio.get_event_loop()\n",
    "            tasks = [\n",
    "                loop.run_in_executor(executor,scrape,item)\n",
    "            \n",
    "                for item in csv_reader]     # the item contains: 0.) index from all aid beneficiaries; 1.) search name; 2.) register number of company; 3.) status to prevent repetitions\n",
    "            for response in await asyncio.gather(*tasks):\n",
    "                pass\n",
    "\n",
    "    time_difference = time.time() - start_time\n",
    "    print(f'Scraping time: %.2f seconds.' % time_difference)\n",
    "\n",
    "def main(): \n",
    "    loop = asyncio.get_event_loop()\n",
    "    future = asyncio.ensure_future(get_data_asynchronous())\n",
    "    loop.run_until_complete(future)\n",
    "\n",
    "main() # \"main\" function starts event loop for asynchronous calls"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_3_9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
